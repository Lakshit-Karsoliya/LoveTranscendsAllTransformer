{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d797f1",
   "metadata": {},
   "source": [
    "# Language Translation Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5bc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "\n",
    "from Encoder import Encoder\n",
    "from PositionalEmbeddings import RoPE\n",
    "from Decoder import Decoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a871b231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "BATCH_SIZE = 32\n",
    "EMB_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c219ee",
   "metadata": {},
   "source": [
    "# Downloading dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc08564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamao/workdir/Transformers/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "Path to dataset files: /home/lamao/.cache/kagglehub/datasets/kuldeepsingharya/english-to-hindi-parallel-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading Dataset\"\"\"\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"kuldeepsingharya/english-to-hindi-parallel-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba908d",
   "metadata": {},
   "source": [
    "# Tokenizer I don't understand this code I asked gpt to help me in tokenizer cuz i am too lazy to write the whole by myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a159549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicians do not have permission to do what needs to be done.\n",
      "राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tokenizers\"\"\"\n",
    "\n",
    "df = pd.read_csv(path+\"/newdata.csv\")\n",
    "print(df[\"english_sentence\"][0])\n",
    "print(df[\"hindi_sentence\"][0])\n",
    "eng_corpus = \" \".join(df['english_sentence'].astype(str).tolist()).lower()\n",
    "hin_corpus = \" \".join(list(df['hindi_sentence'])) \n",
    "\n",
    "eng_trainer = WordLevelTrainer(\n",
    "    vocab_size = 10_000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",\"[SOS]\",\"[EOS]\",]\n",
    ")\n",
    "english_tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "english_tokenizer.pre_tokenizer = Whitespace()\n",
    "english_tokenizer.train_from_iterator([eng_corpus],trainer=eng_trainer)\n",
    "\n",
    "hi_trainer = WordLevelTrainer(\n",
    "    vocab_size = 10_000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",\"[SOS]\",\"[EOS]\",]\n",
    ")\n",
    "hi_tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "hi_tokenizer.pre_tokenizer = Whitespace()\n",
    "hi_tokenizer.train_from_iterator([hin_corpus],trainer=hi_trainer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf22e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset_path, english_tokenizer, hi_tokenizer, max_length):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(dataset_path)\n",
    "        \n",
    "        self.english_tokenizer = english_tokenizer\n",
    "        self.hi_tokenizer = hi_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.pad_id_en = self.english_tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.sos_id_en = self.english_tokenizer.token_to_id(\"[SOS]\")\n",
    "        self.eos_id_en = self.english_tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "        self.pad_id_hi = self.hi_tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.sos_id_hi = self.hi_tokenizer.token_to_id(\"[SOS]\")\n",
    "        self.eos_id_hi = self.hi_tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "        # Filter sentences according to tokenized length\n",
    "        self.english_sentences = []\n",
    "        self.hindi_sentences = []\n",
    "\n",
    "        for en_sent, hi_sent in zip(self.df['english_sentence'].astype(str), self.df['hindi_sentence'].astype(str)):\n",
    "            en_ids = [self.sos_id_en] + self.english_tokenizer.encode(en_sent).ids + [self.eos_id_en]\n",
    "            hi_ids = [self.sos_id_hi] + self.hi_tokenizer.encode(hi_sent).ids + [self.eos_id_hi]\n",
    "\n",
    "            if len(en_ids) <= self.max_length and len(hi_ids) <= self.max_length:\n",
    "                self.english_sentences.append(en_sent)\n",
    "                self.hindi_sentences.append(hi_sent)\n",
    "\n",
    "    def pad_sequence(self, seq, pad_id):\n",
    "        if len(seq) > self.max_length:\n",
    "            seq = seq[:self.max_length]\n",
    "        else:\n",
    "            seq = seq + [pad_id] * (self.max_length - len(seq))\n",
    "        return seq\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Encode and add SOS/EOS\n",
    "        english_ids = [self.sos_id_en] + self.english_tokenizer.encode(self.english_sentences[index].lower()).ids + [self.eos_id_en]\n",
    "        english_ids = self.pad_sequence(english_ids, self.pad_id_en)\n",
    "\n",
    "        encoder_input = english_ids\n",
    "\n",
    "        decoder_input = self.pad_sequence(\n",
    "            [self.sos_id_hi] + self.hi_tokenizer.encode(self.hindi_sentences[index]).ids,\n",
    "            self.pad_id_hi\n",
    "        )\n",
    "        decoder_output = self.pad_sequence(\n",
    "            self.hi_tokenizer.encode(self.hindi_sentences[index]).ids + [self.eos_id_hi],\n",
    "            self.pad_id_hi\n",
    "        )\n",
    "\n",
    "        return torch.tensor(encoder_input), torch.tensor(decoder_input), torch.tensor(decoder_output)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b68d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(path+\"/newdata.csv\",english_tokenizer,hi_tokenizer,max_length=MAX_LEN)\n",
    "dataloader = DataLoader(dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37306a",
   "metadata": {},
   "source": [
    "# Model Architecture (USING Rotatory positional Encodding , not sin cos one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1a7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "    def __init__(self, emb_dim,hidden_dim,n_heads,max_len,num_encoders,num_decoders,input_vocab_size,out_vocab_size):\n",
    "        super().__init__()\n",
    "        rope = RoPE(emb_dim=emb_dim,num_heads=n_heads,seq_len=max_len,device=DEVICE)\n",
    "        RoPE_Precomputed_Angles = rope.precompute_angles()\n",
    "        self.en_embedding = nn.Embedding(input_vocab_size,embedding_dim=emb_dim)\n",
    "        self.hi_embedding = nn.Embedding(out_vocab_size,embedding_dim=emb_dim)\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [Encoder(emb_dim=emb_dim,hidden_dim=hidden_dim,n_heads=n_heads,RoPE=True,RoPE_Precomputed_Angles=RoPE_Precomputed_Angles) for i in range(num_encoders)]\n",
    "        )\n",
    "        self.decoders = nn.ModuleList(\n",
    "            [Decoder(emb_dim=emb_dim,hidden_dim=hidden_dim,n_heads=n_heads,RoPE=True,RoPE_Precomputed_Angles=RoPE_Precomputed_Angles) for i in range(num_decoders)]\n",
    "        )\n",
    "        self.generation_head = nn.Sequential(\n",
    "            nn.Linear(emb_dim,emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim,out_vocab_size)\n",
    "        )\n",
    "    def forward(self,encoder_input , decoder_input):\n",
    "        encoder_input = self.en_embedding(encoder_input)\n",
    "        for encoder in self.encoders:\n",
    "            encoder_input = encoder(encoder_input)\n",
    "        encoder_out = encoder_input\n",
    "        decoder_input = self.hi_embedding(decoder_input)\n",
    "        for decoder in self.decoders:\n",
    "            decoder_input = decoder(decoder_input,encoder_out)\n",
    "        decoder_out = decoder_input\n",
    "        out = self.generation_head(decoder_out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eab6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TranslationModel(\n",
    "        emb_dim=EMB_DIM,\n",
    "        hidden_dim=EMB_DIM,\n",
    "        n_heads=NUM_HEADS,\n",
    "        max_len=MAX_LEN,\n",
    "        num_encoders=6,\n",
    "        num_decoders=6,\n",
    "        input_vocab_size=english_tokenizer.get_vocab_size(),\n",
    "        out_vocab_size=hi_tokenizer.get_vocab_size()\n",
    "    )\n",
    "model = model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff84b4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamao/workdir/Transformers/env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-5\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,factor=0.1,patience=1,min_lr=1e-6,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eda74e",
   "metadata": {},
   "source": [
    "## 25 epochs on 1e-4 lr and 25 on le-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2d80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training loop\n",
    "\"\"\"\n",
    "\n",
    "EPOCHS=100\n",
    "\n",
    "if os.path.exists(\"checkpoints/3_LanguageTranslation.pt\"):\n",
    "    model.load_state_dict(torch.load(\"checkpoints/3_LanguageTranslation.pt\",map_location=DEVICE))\n",
    "else:\n",
    "    losses=[]\n",
    "    loader_len = len(dataloader)\n",
    "    plt.ion()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss=0.0\n",
    "        for batch_num,batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            encoder_input , decoder_input , decoder_out  = batch \n",
    "            encoder_input = encoder_input.to(DEVICE)\n",
    "            decoder_input = decoder_input.to(DEVICE)\n",
    "            decoder_out = decoder_out.to(DEVICE)\n",
    "            out = model(encoder_input,decoder_input)\n",
    "            out = out.view(-1, out.shape[-1])   \n",
    "            decoder_out = decoder_out.view(-1)\n",
    "            loss = criterion(out,decoder_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "            if batch_num % 100 == 0 or batch_num == len(dataloader) - 1:\n",
    "                clear_output(wait=True)\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                plt.plot(losses, label='Batch Loss', alpha=0.7)\n",
    "\n",
    "                if len(losses) > 10:\n",
    "                    ewma = pd.Series(losses).ewm(span=200).mean()\n",
    "                    plt.plot(ewma, label='Smoothed Loss (EWMA)', color='red')\n",
    "\n",
    "                plt.xlabel('Batch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.title(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "            print(f\"\\rEpoch {epoch+1}/{EPOCHS} Batch {batch_num}/{loader_len} loss {loss.item():.6f} last lr : {scheduler.get_last_lr()}\", end='  ', flush=True)\n",
    "            avrage_train_loss = running_loss / len(dataloader)\n",
    "        scheduler.step(avrage_train_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b450ed",
   "metadata": {},
   "source": [
    "# Inference on FP32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3454a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence : How are you?\n",
      "Hindi Translation : आप कैसे हैं ? \n",
      "English Sentence : What is your name?\n",
      "Hindi Translation : आपका नाम क्या है ? \n",
      "English Sentence : Where do you live?\n",
      "Hindi Translation : यहाँ कहाँ रहते हैं ? \n",
      "English Sentence : I am hungry.\n",
      "Hindi Translation : मैं भूख हो गया हूँ । \n",
      "English Sentence : Please help me.\n",
      "Hindi Translation : मेरी मदद करें । \n",
      "English Sentence : Thank you very much.\n",
      "Hindi Translation : बहुत बहुत धन्यवाद ! \n",
      "English Sentence : I love my family.\n",
      "Hindi Translation : मुझे अपने परिवार की याद [UNK] । \n",
      "English Sentence : It is raining today.\n",
      "Hindi Translation : आज इस बात का [UNK] है । \n",
      "English Sentence : I like to read books.\n",
      "Hindi Translation : मैं पढ़ पढ़ना पसंद करती हूँ । \n",
      "English Sentence : The sun is shining.\n",
      "Hindi Translation : सूर्य [UNK] है । \n",
      "English Sentence : Can you speak English?\n",
      "Hindi Translation : क्या आप अंग्रेजी पढ़ सकते हैं ? \n",
      "English Sentence : I am learning Hindi.\n",
      "Hindi Translation : मैं हिन्दी बोलने [UNK] हूँ । \n",
      "English Sentence : Where is the school?\n",
      "Hindi Translation : स्कूल कहाँ है ? \n",
      "English Sentence : This is my friend.\n",
      "Hindi Translation : ये मेरा दोस्त है । \n",
      "English Sentence : I want some water.\n",
      "Hindi Translation : मैं कुछ पानी चाहती हूँ । \n",
      "English Sentence : The sky is blue.\n",
      "Hindi Translation : आकाश नीला होती है । \n",
      "English Sentence : He is my brother.\n",
      "Hindi Translation : ये मेरे [UNK] है । \n",
      "English Sentence : She is very kind.\n",
      "Hindi Translation : वो बहुत [UNK] है । \n",
      "English Sentence : I have a cat.\n",
      "Hindi Translation : मेरे पास एक बिल्ली । \n",
      "English Sentence : Let’s go to the market.\n",
      "Hindi Translation : बाजार में [UNK] जाएँ । \n"
     ]
    }
   ],
   "source": [
    "def pad_sequence(seq, pad_id):\n",
    "    if len(seq) >MAX_LEN:\n",
    "        seq = seq[:MAX_LEN]\n",
    "    else:\n",
    "        seq = seq + [pad_id] * (MAX_LEN - len(seq))\n",
    "    return seq\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"How are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"Where do you live?\",\n",
    "    \"I am hungry.\",\n",
    "    \"Please help me.\",\n",
    "    \"Thank you very much.\",\n",
    "    \"I love my family.\",\n",
    "    \"It is raining today.\",\n",
    "    \"I like to read books.\",\n",
    "    \"The sun is shining.\",\n",
    "    \"Can you speak English?\",\n",
    "    \"I am learning Hindi.\",\n",
    "    \"Where is the school?\",\n",
    "    \"This is my friend.\",\n",
    "    \"I want some water.\",\n",
    "    \"The sky is blue.\",\n",
    "    \"He is my brother.\",\n",
    "    \"She is very kind.\",\n",
    "    \"I have a cat.\",\n",
    "    \"Let’s go to the market.\"\n",
    "]\n",
    "\n",
    "         \n",
    "for text in texts:\n",
    "  print(f\"English Sentence : {text}\")\n",
    "  text = text.lower()\n",
    "  encoder_input = torch.tensor( pad_sequence( [english_tokenizer.token_to_id(\"[SOS]\")]+english_tokenizer.encode(text).ids+[english_tokenizer.token_to_id(\"[EOS]\")] , english_tokenizer.token_to_id(\"[PAD]\") ) ).unsqueeze(0)\n",
    "  decoder_input = torch.tensor( [hi_tokenizer.token_to_id(\"[SOS]\")] ).unsqueeze(0)\n",
    "\n",
    "  # print(encoder_input.shape,decoder_input.shape)\n",
    "\n",
    "  decoder_in = [hi_tokenizer.token_to_id(\"[SOS]\")]\n",
    "  encoder_input = encoder_input.to(DEVICE)\n",
    "  decoder_input = decoder_input.to(DEVICE)\n",
    "  print(\"Hindi Translation : \",end='')\n",
    "  for i in range(20):\n",
    "    decoder_input = torch.tensor(decoder_in).to(DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      out = model(encoder_input , decoder_input)\n",
    "\n",
    "    probs = torch.softmax(out,dim=-1)\n",
    "    preds = torch.argmax(out,dim=-1)\n",
    "    preds = preds.detach().cpu()\n",
    "    predicted_word = hi_tokenizer.id_to_token(preds[0][-1])\n",
    "    decoder_in.append(preds[0][-1])\n",
    "    if predicted_word==\"[EOS]\":break\n",
    "    print(predicted_word,end=' ')\n",
    "  print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4679e5",
   "metadata": {},
   "source": [
    "# MOdel size is around 150 mb -> quantizing to int8 (67 mb)\n",
    "\n",
    "aah not much size reduction, next time use QAT instead of quantizing linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f48e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.quantization import quantize_dynamic\n",
    "model = TranslationModel(\n",
    "        emb_dim=EMB_DIM,\n",
    "        hidden_dim=EMB_DIM,\n",
    "        n_heads=NUM_HEADS,\n",
    "        max_len=MAX_LEN,\n",
    "        num_encoders=6,\n",
    "        num_decoders=6,\n",
    "        input_vocab_size=english_tokenizer.get_vocab_size(),\n",
    "        out_vocab_size=hi_tokenizer.get_vocab_size()\n",
    "    )\n",
    "model.load_state_dict(torch.load(\"checkpoints/3_LanguageTranslation.pt\",map_location='cpu'))\n",
    "model.eval()\n",
    "quantized_model = quantize_dynamic(\n",
    "    model, \n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "torch.save(quantized_model.state_dict(), \"checkpoints/3_LanguageTranslationINT8.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f720f0",
   "metadata": {},
   "source": [
    "# Inferencing with int8 one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamao/workdir/Transformers/env/lib/python3.12/site-packages/torch/_utils.py:410: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence : How are you?\n",
      "Hindi Translation : आप कौन हैं ? \n",
      "English Sentence : What is your name?\n",
      "Hindi Translation : आपका नाम क्या है ? \n",
      "English Sentence : Where do you live?\n",
      "Hindi Translation : यहाँ कहाँ रहते हैं ? \n",
      "English Sentence : I am hungry.\n",
      "Hindi Translation : मैं [UNK] महसूस कर रही हूँ । \n",
      "English Sentence : Please help me.\n",
      "Hindi Translation : मेरी मदद करें । \n",
      "English Sentence : Thank you very much.\n",
      "Hindi Translation : बहुत बहुत धन्यवाद । \n",
      "English Sentence : I love my family.\n",
      "Hindi Translation : मुझे अपने परिवार की याद [UNK] । \n",
      "English Sentence : It is raining today.\n",
      "Hindi Translation : आज इस बात पर [UNK] है । \n",
      "English Sentence : I like to read books.\n",
      "Hindi Translation : मैं पढ़ पढ़ना पसंद करती हूँ । \n",
      "English Sentence : The sun is shining.\n",
      "Hindi Translation : सूर्य [UNK] है । \n",
      "English Sentence : Can you speak English?\n",
      "Hindi Translation : क्या आप अंग्रेजी पढ़ सकते हैं ? \n",
      "English Sentence : I am learning Hindi.\n",
      "Hindi Translation : मैं हिन्दी बोलने [UNK] हूँ । \n",
      "English Sentence : Where is the school?\n",
      "Hindi Translation : स्कूल कहाँ है ? \n",
      "English Sentence : This is my friend.\n",
      "Hindi Translation : ये मेरा दोस्त है । \n",
      "English Sentence : I want some water.\n",
      "Hindi Translation : मैं कुछ पानी चाहती हूँ । \n",
      "English Sentence : The sky is blue.\n",
      "Hindi Translation : आकाश नीला होती है । \n",
      "English Sentence : He is my brother.\n",
      "Hindi Translation : ये मेरे [UNK] है । \n",
      "English Sentence : She is very kind.\n",
      "Hindi Translation : वो बहुत [UNK] है । \n",
      "English Sentence : I have a cat.\n",
      "Hindi Translation : मेरे पास एक बिल्ली । \n",
      "English Sentence : Let’s go to the market.\n",
      "Hindi Translation : बाजार में [UNK] लेना देखना [UNK] हैं । \n"
     ]
    }
   ],
   "source": [
    "model = TranslationModel(\n",
    "        emb_dim=EMB_DIM,\n",
    "        hidden_dim=EMB_DIM,\n",
    "        n_heads=NUM_HEADS,\n",
    "        max_len=MAX_LEN,\n",
    "        num_encoders=6,\n",
    "        num_decoders=6,\n",
    "        input_vocab_size=english_tokenizer.get_vocab_size(),\n",
    "        out_vocab_size=hi_tokenizer.get_vocab_size()\n",
    "    )\n",
    "model.eval()\n",
    "quantized_model = quantize_dynamic(\n",
    "    model, \n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "quantized_model.load_state_dict(torch.load(\"checkpoints/3_LanguageTranslationINT8.pt\", map_location=\"cpu\"))\n",
    "quantized_model.eval()\n",
    "\n",
    "def pad_sequence(seq, pad_id):\n",
    "    if len(seq) >MAX_LEN:\n",
    "        seq = seq[:MAX_LEN]\n",
    "    else:\n",
    "        seq = seq + [pad_id] * (MAX_LEN - len(seq))\n",
    "    return seq\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"How are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"Where do you live?\",\n",
    "    \"I am hungry.\",\n",
    "    \"Please help me.\",\n",
    "    \"Thank you very much.\",\n",
    "    \"I love my family.\",\n",
    "    \"It is raining today.\",\n",
    "    \"I like to read books.\",\n",
    "    \"The sun is shining.\",\n",
    "    \"Can you speak English?\",\n",
    "    \"I am learning Hindi.\",\n",
    "    \"Where is the school?\",\n",
    "    \"This is my friend.\",\n",
    "    \"I want some water.\",\n",
    "    \"The sky is blue.\",\n",
    "    \"He is my brother.\",\n",
    "    \"She is very kind.\",\n",
    "    \"I have a cat.\",\n",
    "    \"Let’s go to the market.\"\n",
    "]\n",
    "\n",
    "         \n",
    "for text in texts:\n",
    "  print(f\"English Sentence : {text}\")\n",
    "  text = text.lower()\n",
    "  encoder_input = torch.tensor( pad_sequence( [english_tokenizer.token_to_id(\"[SOS]\")]+english_tokenizer.encode(text).ids+[english_tokenizer.token_to_id(\"[EOS]\")] , english_tokenizer.token_to_id(\"[PAD]\") ) ).unsqueeze(0)\n",
    "  decoder_input = torch.tensor( [hi_tokenizer.token_to_id(\"[SOS]\")] ).unsqueeze(0)\n",
    "\n",
    "  # print(encoder_input.shape,decoder_input.shape)\n",
    "\n",
    "  decoder_in = [hi_tokenizer.token_to_id(\"[SOS]\")]\n",
    "  encoder_input = encoder_input.to(DEVICE)\n",
    "  decoder_input = decoder_input.to(DEVICE)\n",
    "  print(\"Hindi Translation : \",end='')\n",
    "  for i in range(20):\n",
    "    decoder_input = torch.tensor(decoder_in).to(DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      out = quantized_model(encoder_input , decoder_input)\n",
    "\n",
    "    probs = torch.softmax(out,dim=-1)\n",
    "    preds = torch.argmax(out,dim=-1)\n",
    "    preds = preds.detach().cpu()\n",
    "    predicted_word = hi_tokenizer.id_to_token(preds[0][-1])\n",
    "    decoder_in.append(preds[0][-1])\n",
    "    if predicted_word==\"[EOS]\":break\n",
    "    print(predicted_word,end=' ')\n",
    "  print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb16397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
