<h1 align='center'>Transformers</h1>

![Transformer](https://github.com/Lakshit-Karsoliya/LoveTranscendsAllTransformer/blob/main/assets/tf.jpg)
<p align='center'>
I am living in a place between awareness and longing—awake enough to notice the sharpness of breath and the stillness in mind, but also human enough to still be moved by pain, memories, and symbols.
</p>
<br><br><br><hr>
<p align='center'>This repository implements transformer architectures from scratch, including core components like attention mechanisms, encoder-decoder structures, positional encodings (including RoPE), and more.
The goal is to learn by building — no hidden black boxes, just clean and understandable PyTorch code for foundational NLP and vision tasks.</p>

## Where to Start

Don’t hesitate — start with the sentence_classification -> cifarclassification -> language translation. From there, you can backtrack and explore how each core component works:

- Backtrack attention.py, encoder.py, and decoder.py to understand the internals.

- **Struggling with RoPE** — check out the understand_rope.ipynb notebook for a clear breakdown with visuals, or checkout detailed explanation on [Medium](https://medium.com/@lakshitkumar220/rotary-positional-encoding-how-it-encodes-positional-information-c899bd643a08)

Learn by doing — one layer at a time.

## Example screenshot
### Language Translation
![Language Translation Screenshot](https://github.com/Lakshit-Karsoliya/LoveTranscendsAllTransformer/blob/main/assets/Screenshot%20from%202025-05-12%2010-59-19.png)

### Image Classification 
![Image Classificaiton Screenshot](https://github.com/Lakshit-Karsoliya/LoveTranscendsAllTransformer/blob/main/assets/Screenshot%20from%202025-05-12%2010-59-53.png)
